\documentclass[ascii]{abook}

% Allowed text encodings: ascii, koi8-r, koi8-u, cp1251, utf-8.

% Some frequently used packages are loaded by the class.
% If you need a package that is not loaded, you should load it here.

\begin{document}


% Please use this template to prepare your abstract.
% Please note that it should not exceed one page.
% The file should be renamed using family names of first two coauthors as in the following example: {Doe-Doe.tex}.

% Both TeX and PDF files of your abstract should be sent to {xiv.iacu@gmail.com}

\begin{abstract}

% You should define custom commands here.

% The language of the abstract.
% Allowed choices: english, ukrainian.
\selectlanguage{english}

\title{MoA+: Mixture of Autoencoders with Various Concentrations for Enhanced Image Clustering}

% Syntax of the \author command:
% \author[Short name]{Full name}{Affiliation}{e-mail}
% <Short name> --- initials of the first name and the last name.
% It will be used in the table of contents and the index.
% <Full name> --- name in the complete form preferred by the author, i.e. the first name and the last name.
% <Affiliation> --- author's affiliation.
% <e-mail> --- author's e-mail address.

% <Affiliation> and <e-mail> may be left blank.

% You should use \\* (star is mandatory) in the arguments of
% \title and \author to insert line break.

\author[V. ~Miroshnychenko]{Vitaliy Miroshnychenko}
{Taras Shevchenko National University of Kyiv, Kyiv, Ukraine.}
{vitaliy.miroshnychenko@knu.ua}
\author[V.~Tunik]{Vadym Tunik}
{Taras Shevchenko National University of Kyiv, Kyiv, Ukraine.}
{tunik.vadym@knu.ua}

\maketitle

% The text of the abstract.
% Avoid bold fonts in the text.

In this paper, we consider the improvement of the vanilla Mixture of Experts model \cite{hinton1991} in the unsupervised image clustering task by introducing an unbiased loss function based on the idea of a mixture of various concentrations. This addresses expert dominance, enhancing training balance and clustering performance. 

The model is tested on the MNIST dataset, a benchmark of $N$ grayscale handwritten digit images $X_i, i=1..N$, treated as a mixture with $M = 2$ components.

In the MoA+ framework, each expert is a convolutional autoencoder (CAE), similar to simplified U-Net or SegNet, where the $k$-th CAE reconstructs image $X_i$ into $v_{i,k}$ with MSE error. A convolutional gate network assigns probabilities $p_{i,k}$, clustering images by selecting the most suitable CAE.

To prevent expert imbalance, MoA+ uses the $Q$-loss, inspired by ST-MoEâ€™s z-loss \cite{moe2022}. 

$$
\text{$Q$-loss} = \sqrt{ \sum_{k=1}^{M} \left( \sum_{i=1}^{N} p_{i,k} \cdot \text{MSE}(v_{i,k}, X_i) \right)^2 }
$$

This regularizes the gate network, ensuring all experts contribute effectively to the clustering process. 

Compared under identical conditions, the standard MoA achieved a Normalized Mutual Information (NMI) of $\sim0.08$, while MoA+ scored $\sim0.8$, showing significantly better clustering performance.



% References (if you use any).
\vspace{0.2cm}
\hrule
\vspace{0.1cm}
\textbf{References}
\vspace{0.1cm}
\hrule


\begin{thebibliography}{9}
\bibitem{hinton1991}
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., \& Hinton, G. E. 
(1991). 
\emph{Adaptive Mixtures of Local Experts}. 
Neural Computation, 3(1), 79--87. 
\url{https://direct.mit.edu/neco/article/3/1/79/5560/Adaptive-Mixtures-of-Local-Experts}.

\bibitem{moe2022}
Author, A. B. (2022). 
\emph{ST-MoE: Designing Stable and Transferable Sparse Expert Models}. 
arXiv preprint arXiv:2202.08906. 
\url{https://arxiv.org/abs/2202.08906}.
\end{thebibliography}

\end{abstract}

\end{document}
